{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy\n",
    "import math\n",
    "import matplotlib.patches as patches\n",
    "import pylab\n",
    "import shapely.geometry as geometry\n",
    "import pylab as plt\n",
    "from matplotlib.path import Path\n",
    "import scipy.stats\n",
    "import shapefile\n",
    "from matplotlib.path import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in ensemble, interpolated to 5km grid using nearest neighbor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = xr.open_dataset('double_ens/indus_interp.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in synthetic observations. Frequency does not account for \"wet\" snow (aka wet snow is included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = xr.open_dataset('obs_wetsnow_v2.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in uninterpolated ensemble, just for data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = xr.open_dataset('../files_for_pbs/ensemble_final1.nc')\n",
    "ensemble['time'] = check.time\n",
    "del check\n",
    "ensemble_snow = ensemble.snd\n",
    "ensemble_snow = ensemble_snow.load()\n",
    "del ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = ensemble_snow.lon\n",
    "lat = ensemble_snow.lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_snow = ensemble_snow.transpose('ensemble', 'time', 'lat', 'lon')\n",
    "ensemble_snow = ensemble_snow.sel(time=slice('2016-10', '2017-09'))\n",
    "ensemble_snow['ensemble'] = np.arange(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assimilating all observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing weights. Initially, all weights are the same (1). I normalize the weights later, so it doesn't matter that their not 1/n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.ones([340, 300, 100])\n",
    "weights = xr.DataArray(weights, coords = {'lon': lon, 'lat': lat, 'ensemble': np.arange(100)}, dims = ['lon', 'lat', 'ensemble'])\n",
    "weights = weights.transpose('ensemble', 'lat', 'lon')\n",
    "#weights = weights.where(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting and cleaning snow depth observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_SD = obs.SnowDepth_tavg\n",
    "del obs\n",
    "obs_SD = obs_SD.rename({'east_west': 'lon', 'north_south': 'lat'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_SD = obs_SD.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assimilaiton algorithm for assimilating all observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in np.arange(100):\n",
    "    #Loop through each ensemble member \n",
    "    mem = ensemble_snow.sel({'ensemble': j})\n",
    "    #Observation uncertainty has the same standard deviation \n",
    "    #as the uncertainty that I specified when creating the \n",
    "    #synthetic observations \n",
    "    std = (obs_SD + 0.01)*0.30  \n",
    "    #Taking the difference between the ensemble member and \n",
    "    #the syntehic observation \n",
    "    dif = mem - obs_SD \n",
    "    del mem \n",
    "    dif = dif.load()\n",
    "    #Calculating the Likelihood. Std is descirbed above. \n",
    "    #Bias is set to -0.01, because this matches the bias of the\n",
    "    #Sentinel-1 data\n",
    "    lik = scipy.stats.norm.pdf(dif, -0.01, std)\n",
    "    del dif\n",
    "    #Replacing nans with 1 (When calculating joint likelihood, \n",
    "    #the one will have no affect)\n",
    "    lik = np.nan_to_num(lik, nan = 1)\n",
    "    #Replacing zero likelihoods with very small numbers to \n",
    "    #prevent div by zeros \n",
    "    zeros = lik == 0\n",
    "    lik[zeros] = 3.8 * 10**(-322)\n",
    "    #Calculating the joint log-likelihood. \n",
    "    #Using log-likelihood prevents the joint likelihoods from\n",
    "    #collapsing to zero. When using normal joint likelihood\n",
    "    #the numbers become so small, that python turns them\n",
    "    #into zeros. \n",
    "    joint_lik = np.log(lik[0])\n",
    "    for i in np.arange(1, 273):\n",
    "        elem = np.log(lik[i])\n",
    "        joint_lik = joint_lik + elem\n",
    "    #Updating weights \n",
    "    weights[j] = joint_lik\n",
    "    del joint_lik \n",
    "    del lik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, sum up the weights for each ensemble, to caclulate \n",
    "#the normalizing factor \n",
    "log_add = weights.sel({'ensemble':0})\n",
    "for i in np.arange(1, 100):\n",
    "    add = weights.sel({'ensemble': i})\n",
    "    #np.logaddexp adds logs together but somehow doesn't \n",
    "    #round the small numbers to zero, like python does. \n",
    "    log_add = np.logaddexp(log_add, add)\n",
    "#Now we can normalize the weights, still using log \n",
    "normalized_log = weights - log_add\n",
    "#Finally, we exponentiate the weights \n",
    "normalized_all = np.exp(normalized_log)\n",
    "normalized_all = normalized_all.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the weights as a netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_all.to_netcdf('all_assim_weights_wetsnow_v2.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assimilating maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del weights\n",
    "del normalized_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del log_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.ones([340, 300, 100])\n",
    "weights = xr.DataArray(weights, coords = {'lon': lon, 'lat': lat, 'ensemble': np.arange(100)}, dims = ['lon', 'lat', 'ensemble'])\n",
    "weights = weights.transpose('ensemble', 'lat', 'lon')\n",
    "#weights = weights.where(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling zeros with very small numbers (they won't be the maximum, so this nans are still ignored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_SD_fill = obs_SD.fillna(0.0000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the index of the OBSERVATION maximum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_max_ind = obs_SD_fill.argmax('time', skipna = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the OBSERVATION maximum value and calculating the observation \n",
    "uncertainty of this maximum value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_max = obs_SD_fill.max('time', skipna= True)\n",
    "#obs_max = obs_max.where(mask)\n",
    "std = (obs_max + 0.01) * 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(100):\n",
    "    #Looping through each ensemble member\n",
    "    mem = ensemble_snow.sel({'ensemble': i})\n",
    "    #Getting the model SD value, for where the observation\n",
    "    #maximum is. \n",
    "    max_mem = mem[obs_max_ind]\n",
    "    max_mem = max_mem.load()\n",
    "    #Calculating the error and likelihood \n",
    "    error = max_mem - obs_max \n",
    "    lik = scipy.stats.norm.pdf(error, -0.01, std)\n",
    "    #Replacing zeros with very small numbers to avoid \n",
    "    #div by zero.\n",
    "    zeros = lik == 0\n",
    "    lik[zeros] = 3.8 * 10**(-322)\n",
    "    weights[i] = np.log(lik)\n",
    "#Normalizing weights, as before\n",
    "log_add = weights.sel({'ensemble':0})\n",
    "for i in np.arange(1, 100):\n",
    "    add = weights.sel({'ensemble': i})\n",
    "    log_add = np.logaddexp(log_add, add)\n",
    "normalized_log = weights - log_add\n",
    "normalized_max = np.exp(normalized_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving weights as netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_max.to_netcdf('max_assim_weights_wetsnow_v2.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reanalysis-kernel",
   "language": "python",
   "name": "reanalysis-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
